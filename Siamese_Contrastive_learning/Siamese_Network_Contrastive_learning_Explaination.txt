This code implements a Siamese neural network with hyperparameter optimization using Hyperopt to learn refined embeddings for protein-ligand pairs through contrastive learning. The input consists of a combined feature matrix where the first 500 columns represent ligand descriptors (e.g., molecular fingerprints) and subsequent columns contain protein features, which are standardized using StandardScaler and converted to PyTorch tensors. The system generates balanced training pairs through a create_pairs function that creates positive pairs (samples from the same class, labeled 0/1/2 for LDHA/LDHB/LDHC isoforms) and negative pairs (5:1 ratio by default) to ensure robust contrastive learning. The Siamese architecture uses shared-weight fully connected layers with ReLU activations, progressively reducing dimensions from the input size to 128-D embeddings through three linear layers (input_dim → hidden_size → 256 → 128). The contrastive loss function incorporates a tunable margin parameter and includes a small epsilon (1e-6) for numerical stability, computing loss as (1-Y)D² + Ymax(margin-D,0)² where Y is the pair label and D is the Euclidean distance between embeddings. Hyperparameter optimization via Tree-structured Parzen Estimator (TPE) searches over hidden sizes (128/256/512), learning rates (log-uniform 1e-5 to 1e-3), and margin values (uniform 0.5-2.0) across 10 evaluations, minimizing the average epoch loss. The final model trains for 50 epochs with optimized parameters, producing 128-dimensional embeddings that better separate the isoforms, which are saved alongside the model weights. Key improvements include balanced negative sampling, NaN prevention in loss calculations, and architectural optimization through hyperparameter tuning, making this suitable for downstream tasks like interaction prediction or visualization. The refined embeddings are stored in CSV format with original isoform labels, while the model is saved as a .pth file for inference.
